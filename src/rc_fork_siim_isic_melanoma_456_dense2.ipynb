{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "rc-fork-siim-isic-melanoma-456-dense2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORUyQ7sWvM33",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bd39cb89-859a-48a6-e049-ca41bb6d3094"
      },
      "source": [
        "'''\n",
        "Efn-B4 Model\n",
        "384x384 image data (Triple Stratified Data)\n",
        "40 epochs\n",
        "3 Folds\n",
        "Early Stopping\n",
        "Tabular Data + Image Data\n",
        "Focal Loss\n",
        "Using External Data\n",
        "'''"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nEfn-B4 Model\\n384x384 image data (Triple Stratified Data)\\n40 epochs\\n3 Folds\\nEarly Stopping\\nTabular Data + Image Data\\nFocal Loss\\nUsing External Data\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQZjr9N0vQO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Kaggle = False\n",
        "Colab = !Kaggle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWCB1AK8Tb9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "0f20baed-15b6-452d-ddf7-a85afd724648"
      },
      "source": [
        "!pip install tensorflow~=2.2.0 tensorflow_gcs_config~=2.2.0\n",
        "import tensorflow as tf\n",
        "import requests\n",
        "import os\n",
        "resp = requests.post(\"http://{}:8475/requestversion/{}\".format(os.environ[\"COLAB_TPU_ADDR\"].split(\":\")[0], tf.__version__))\n",
        "if resp.status_code != 200:\n",
        "  print(\"Failed to switch the TPU to TF {}\".format(version))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow~=2.2.0 in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: tensorflow_gcs_config~=2.2.0 in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (3.12.4)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.4.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (0.9.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.30.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (2.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.18.5)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.6.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (2.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (0.34.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow~=2.2.0) (49.2.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (2.10)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI5nq3javUhc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dcb44261-d1ee-4e89-a477-736e46aef5eb"
      },
      "source": [
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "if Colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    path = \"/content/drive/My Drive\"\n",
        "    os.chdir(path)\n",
        "    os.listdir(path)\n",
        "else:\n",
        "    from kaggle_datasets import KaggleDatasets"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N53GA2cVvW9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if Kaggle:\n",
        "    BASEPATH = \"../input/siim-isic-melanoma-classification\"\n",
        "    outdir = '.'\n",
        "else:\n",
        "    PATH = 'siim/'\n",
        "    BASEPATH = PATH + 'siim-isic-melanoma-classification'\n",
        "    outdir = Path(PATH+'res')\n",
        "    if not os.path.exists(outdir):\n",
        "        os.mkdir(outdir)\n",
        "    outdir = Path(PATH+'res/efficientnet-res')\n",
        "    if not os.path.exists(outdir):\n",
        "        os.mkdir(outdir)\n",
        "    MODELNAME = \"EF5-512\"\n",
        "    VERSION = '{}'.format(MODELNAME)\n",
        "    outdir = os.path.join(outdir, VERSION)\n",
        "    if not os.path.exists(outdir):\n",
        "        os.mkdir(outdir)\n",
        "    # from datetime import datetime, timedelta\n",
        "    # dateTimeObj = datetime.now()\n",
        "    # timestampStr = dateTimeObj.strftime(\"%d-%b-%Y-%H\")\n",
        "    timestampStr = 'all-kfold-hairaug-456-norm-meta-dense'\n",
        "    outdir = os.path.join(outdir, timestampStr)\n",
        "    if not os.path.exists(outdir):\n",
        "        os.mkdir(outdir)   "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "MzsegUbdvM37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q efficientnet >> /dev/null\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import tensorflow as tf\n",
        "# from kaggle_datasets import KaggleDatasets\n",
        "import efficientnet.tfkeras as efn\n",
        "import dill\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "rZc3Gg8ivM3-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "31909b96-87fd-49ad-84c0-6c200643d955"
      },
      "source": [
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on TPU  grpc://10.71.222.18:8470\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.71.222.18:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.71.222.18:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "REPLICAS:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7a83GabwCQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## melanoma dataset\n",
        "gcs_path = []\n",
        "gcs_path.append('gs://kds-2b28e4519749435c2fb543681ca66aaee6159d9fcc906d5b26f8916e') #192\n",
        "gcs_path.append('gs://kds-f8af4bf2f6c765dbd327683a170e364332a1750658e910153e98d0d5') #256\n",
        "gcs_path.append('gs://kds-71b679af6661316328cfa122f5342b4a84fe0c7c1135d809fbee8ca4') #384\n",
        "gcs_path.append('gs://kds-458e4ec4a30e5d927b21efd0633fb2c0a091245fe8db3c415fde5da6') #512\n",
        "gcs_path.append('gs://kds-cfb16ed5f55adb5ab35a75a2fe74c3dc11a4b869dc8cfb3d9b1759e1') #768\n",
        "\n",
        "## isic dataset\n",
        "gcs_path2 = []\n",
        "gcs_path2.append('gs://kds-c9c10ac29c6818441da475638226dfcf3d12806980a23f179a21d826') #192-2018\n",
        "gcs_path2.append('gs://kds-436e76b7e255e4499d5d11da9d7ab87646e271e08ec1d546f310ec00') #256-2018\n",
        "gcs_path2.append('gs://kds-6f9d2143e7fe97eccac0219c1aacb107a234b440670f250683ba40ee') #384-2018\n",
        "gcs_path2.append('gs://kds-203bea6cc14c2866536ac303b09a6e77a6b3d0a371e96b680836433c') #512-2018\n",
        "gcs_path2.append('gs://kds-79d9c56dcae7978f98a6bbe8318ac6866ba4778fc95c5871fcfd0f03') #768-2019 \n",
        "\n",
        "gcs_path_dict = {192:0,256:1,384:2,512:3,768:4}\n",
        "\n",
        "gcs_path_hairs = 'gs://kds-0f1845648605e1d5c6a531816acb4e993458e02c32d8f4d5bc6bf4cf'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2VtfHdRxwsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Roman's images of hairs\n",
        "hair_images=tf.io.gfile.glob(gcs_path_hairs + '/*.png')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUmHPbQyvM4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For tf.dataset\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "n_max = 10\n",
        "# Data access\n",
        "img_size = 456\n",
        "# IMG_SIZES = [img_size,img_size,img_size,img_size,img_size]\n",
        "img_read_size = 512\n",
        "# IMG_READ_SIZES = [img_read_size,img_read_size,img_read_size,img_read_size,img_read_size]\n",
        "img_crop_size = 452\n",
        "if Kaggle:\n",
        "    GCS_PATH = KaggleDatasets().get_gcs_path('melanoma-{}x{}'.format(img_read_size, img_read_size))\n",
        "    GCS_PATH2 = KaggleDatasets().get_gcs_path('isic2019-{}x{}'.format(img_read_size, img_read_size))\n",
        "else:    \n",
        "    GCS_PATH = gcs_path[gcs_path_dict[img_read_size]]\n",
        "    GCS_PATH2 = gcs_path2[gcs_path_dict[img_read_size]]\n",
        "\n",
        "# Configuration\n",
        "EPOCHS = 40\n",
        "BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n",
        "AUG_BATCH = BATCH_SIZE\n",
        "\n",
        "# Seed\n",
        "SEED = 42\n",
        "# Learning rate\n",
        "LR = 0.0003\n",
        "\n",
        "# training filenames directory\n",
        "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train*.tfrec')\n",
        "TRAINING_FILENAMES_OLD_DATA = tf.io.gfile.glob(GCS_PATH2 + '/train%.2i*.tfrec'%(2*x) for x in range(15))\n",
        "\n",
        "# test filenames directory\n",
        "TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test*.tfrec')\n",
        "# submission file\n",
        "SUB = pd.read_csv('siim/siim-isic-melanoma-classification/sample_submission.csv')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGG4wcsJvM4D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e7064075-6965-40d1-9fa6-37f6c0babb57"
      },
      "source": [
        "len(TRAINING_FILENAMES)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abeuh32NvM4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAINING_FILENAMES += TRAINING_FILENAMES_OLD_DATA"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8vWoZMJvM4L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14fb7f89-7c8b-4f8b-fbe3-40ec58716a62"
      },
      "source": [
        "len(TRAINING_FILENAMES)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m9tvrdSvM4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Taken from following\n",
        "#https://github.com/yu4u/cutout-random-erasing/blob/master/random_eraser.py\n",
        "'''\n",
        "Parameters\n",
        "Parameters are fully configurable as:\n",
        "\n",
        "get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3,\n",
        "                  v_l=0, v_h=255, pixel_level=False)\n",
        "p        : the probability that random erasing is performed\n",
        "s_l, s_h : minimum / maximum proportion of erased area against input image\n",
        "r_1, r_2 : minimum / maximum aspect ratio of erased area\n",
        "v_l, v_h : minimum / maximum value for erased area\n",
        "pixel_level : pixel-level randomization for erased area\n",
        "''' \n",
        "def get_random_eraser(p=0.6, s_l=0.02, s_h=0.4, r_1=0.2, r_2=1/0.2, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :].assign(tf.Variable(c))\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul-EaSbEvM4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cutout_aug = get_random_eraser(s_h=0.2)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfmg9r60vM4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_erasing(img, probability = 0.5, sl = 0.02, sh = 0.2, r1 = 0.1):\n",
        "    '''\n",
        "    img is a 3-D variable (ex: tf.Variable(image, validate_shape=False) ) and  HWC order\n",
        "    '''\n",
        "    # HWC order\n",
        "    height = tf.shape(img)[0]\n",
        "    width = tf.shape(img)[1]\n",
        "    channel = tf.shape(img)[2]\n",
        "    area = tf.cast(width*height, tf.float32)\n",
        "\n",
        "    erase_area_low_bound = tf.cast(tf.round(tf.sqrt(sl * area * r1)), tf.int32)\n",
        "    erase_area_up_bound = tf.cast(tf.round(tf.sqrt((sh * area) / r1)), tf.int32)\n",
        "    h_upper_bound = tf.minimum(erase_area_up_bound, height)\n",
        "    w_upper_bound = tf.minimum(erase_area_up_bound, width)\n",
        "\n",
        "    h = tf.random.uniform([], erase_area_low_bound, h_upper_bound, tf.int32)\n",
        "    w = tf.random.uniform([], erase_area_low_bound, w_upper_bound, tf.int32)\n",
        "\n",
        "    x1 = tf.random.uniform([], 0, height+1 - h, tf.int32)\n",
        "    y1 = tf.random.uniform([], 0, width+1 - w, tf.int32)\n",
        "\n",
        "    erase_area = tf.cast(tf.random.uniform([h, w, channel], 0, 255, tf.int32), tf.uint8)\n",
        "\n",
        "    erasing_img = img[x1:x1+h, y1:y1+w, :].assign(erase_area)\n",
        "\n",
        "    return tf.cond(tf.random.uniform([], 0, 1) > probability, lambda: img, lambda: erasing_img)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rUXtokovM4V",
        "colab_type": "text"
      },
      "source": [
        "# CUTOUT Aug"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKEROoh7vM4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_cut(image, inv_mat, image_shape):\n",
        "\n",
        "    h, w, c = image_shape\n",
        "    cx, cy = w//2, h//2\n",
        "\n",
        "    new_xs = tf.repeat( tf.range(-cx, cx, 1), h)\n",
        "    new_ys = tf.tile( tf.range(-cy, cy, 1), [w])\n",
        "    new_zs = tf.ones([h*w], dtype=tf.int32)\n",
        "\n",
        "    old_coords = tf.matmul(inv_mat, tf.cast(tf.stack([new_xs, new_ys, new_zs]), tf.float32))\n",
        "    old_coords_x, old_coords_y = tf.round(old_coords[0, :] + w//2), tf.round(old_coords[1, :] + h//2)\n",
        "\n",
        "    clip_mask_x = tf.logical_or(old_coords_x<0, old_coords_x>w-1)\n",
        "    clip_mask_y = tf.logical_or(old_coords_y<0, old_coords_y>h-1)\n",
        "    clip_mask = tf.logical_or(clip_mask_x, clip_mask_y)\n",
        "\n",
        "    old_coords_x = tf.boolean_mask(old_coords_x, tf.logical_not(clip_mask))\n",
        "    old_coords_y = tf.boolean_mask(old_coords_y, tf.logical_not(clip_mask))\n",
        "    new_coords_x = tf.boolean_mask(new_xs+cx, tf.logical_not(clip_mask))\n",
        "    new_coords_y = tf.boolean_mask(new_ys+cy, tf.logical_not(clip_mask))\n",
        "\n",
        "    old_coords = tf.cast(tf.stack([old_coords_y, old_coords_x]), tf.int32)\n",
        "    new_coords = tf.cast(tf.stack([new_coords_y, new_coords_x]), tf.int64)\n",
        "    rotated_image_values = tf.gather_nd(image, tf.transpose(old_coords))\n",
        "    rotated_image_channel = list()\n",
        "    for i in range(c):\n",
        "        vals = rotated_image_values[:,i]\n",
        "        sparse_channel = tf.SparseTensor(tf.transpose(new_coords), vals, [h, w])\n",
        "        rotated_image_channel.append(tf.sparse.to_dense(sparse_channel, default_value=0, validate_indices=False))\n",
        "\n",
        "    return tf.transpose(tf.stack(rotated_image_channel), [1,2,0])\n",
        "\n",
        "def random_rotate(image, angle, image_shape):\n",
        "\n",
        "    def get_rotation_mat_inv(angle):\n",
        "          #transform to radian\n",
        "        angle = math.pi * angle / 180\n",
        "\n",
        "        cos_val = tf.math.cos(angle)\n",
        "        sin_val = tf.math.sin(angle)\n",
        "        one = tf.constant([1], tf.float32)\n",
        "        zero = tf.constant([0], tf.float32)\n",
        "\n",
        "        rot_mat_inv = tf.concat([cos_val, sin_val, zero,\n",
        "                                     -sin_val, cos_val, zero,\n",
        "                                     zero, zero, one], axis=0)\n",
        "        rot_mat_inv = tf.reshape(rot_mat_inv, [3,3])\n",
        "\n",
        "        return rot_mat_inv\n",
        "    angle = float(angle) * tf.random.normal([1],dtype='float32')\n",
        "    rot_mat_inv = get_rotation_mat_inv(angle)\n",
        "    return transform_cut(image, rot_mat_inv, image_shape)\n",
        "\n",
        "\n",
        "def GridMask(image_height, image_width, d1, d2, rotate_angle=1, ratio=0.2):\n",
        "\n",
        "    h, w = image_height, image_width\n",
        "    hh = int(np.ceil(np.sqrt(h*h+w*w)))\n",
        "    hh = hh+1 if hh%2==1 else hh\n",
        "    d = tf.random.uniform(shape=[], minval=d1, maxval=d2, dtype=tf.int32)\n",
        "    l = tf.cast(tf.cast(d,tf.float32)*ratio+0.5, tf.int32)\n",
        "\n",
        "    st_h = tf.random.uniform(shape=[], minval=0, maxval=d, dtype=tf.int32)\n",
        "    st_w = tf.random.uniform(shape=[], minval=0, maxval=d, dtype=tf.int32)\n",
        "\n",
        "    y_ranges = tf.range(-1 * d + st_h, -1 * d + st_h + l)\n",
        "    x_ranges = tf.range(-1 * d + st_w, -1 * d + st_w + l)\n",
        "\n",
        "    for i in range(0, hh//d+1):\n",
        "        s1 = i * d + st_h\n",
        "        s2 = i * d + st_w\n",
        "        y_ranges = tf.concat([y_ranges, tf.range(s1,s1+l)], axis=0)\n",
        "        x_ranges = tf.concat([x_ranges, tf.range(s2,s2+l)], axis=0)\n",
        "\n",
        "    x_clip_mask = tf.logical_or(x_ranges <0 , x_ranges > hh-1)\n",
        "    y_clip_mask = tf.logical_or(y_ranges <0 , y_ranges > hh-1)\n",
        "    clip_mask = tf.logical_or(x_clip_mask, y_clip_mask)\n",
        "\n",
        "    x_ranges = tf.boolean_mask(x_ranges, tf.logical_not(clip_mask))\n",
        "    y_ranges = tf.boolean_mask(y_ranges, tf.logical_not(clip_mask))\n",
        "\n",
        "    hh_ranges = tf.tile(tf.range(0,hh), [tf.cast(tf.reduce_sum(tf.ones_like(x_ranges)), tf.int32)])\n",
        "    x_ranges = tf.repeat(x_ranges, hh)\n",
        "    y_ranges = tf.repeat(y_ranges, hh)\n",
        "\n",
        "    y_hh_indices = tf.transpose(tf.stack([y_ranges, hh_ranges]))\n",
        "    x_hh_indices = tf.transpose(tf.stack([hh_ranges, x_ranges]))\n",
        "\n",
        "    y_mask_sparse = tf.SparseTensor(tf.cast(y_hh_indices, tf.int64),  tf.zeros_like(y_ranges), [hh, hh])\n",
        "    y_mask = tf.sparse.to_dense(y_mask_sparse, 1, False)\n",
        "\n",
        "    x_mask_sparse = tf.SparseTensor(tf.cast(x_hh_indices, tf.int64), tf.zeros_like(x_ranges), [hh, hh])\n",
        "    x_mask = tf.sparse.to_dense(x_mask_sparse, 1, False)\n",
        "\n",
        "    mask = tf.expand_dims( tf.clip_by_value(x_mask + y_mask, 0, 1), axis=-1)\n",
        "\n",
        "    #mask = random_rotate(mask, rotate_angle, [hh, hh, 1])\n",
        "    mask = tf.image.crop_to_bounding_box(mask, (hh-h)//2, (hh-w)//2, image_height, image_width)\n",
        "\n",
        "    return mask\n",
        "\n",
        "def apply_grid_mask(image, image_shape):\n",
        "    mask = GridMask(image_shape[0],\n",
        "                    image_shape[1],\n",
        "                    50,100,\n",
        "                    0.2)\n",
        "    \n",
        "    if image_shape[-1] == 3:\n",
        "        mask = tf.concat([mask, mask, mask], axis=-1)\n",
        "\n",
        "    return image * tf.cast(mask, tf.float32)\n",
        "\n",
        "def random_cuout_aug(image):\n",
        "    if tf.random.uniform(shape=[], minval=0.0, maxval=1.0) >=0.5:\n",
        "        image = apply_grid_mask(image, (img_size, img_size, 3))\n",
        "    return tf.cast(image, tf.float32)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhWgsq3ex2gi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hair_images_tf=tf.convert_to_tensor(hair_images)\n",
        "scale=tf.cast(img_size/256, dtype=tf.int32)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGVQW7NWx29u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hair_aug(image):\n",
        "    # Copy the input image, so it won't be changed\n",
        "    img=tf.identity(image) \n",
        "    # Randomly choose the number of hairs to augment (up to n_max)\n",
        "    n_hairs = tf.random.uniform(shape=[], maxval=tf.constant(n_max)+1, \n",
        "                                dtype=tf.int32)\n",
        "    \n",
        "    im_height=tf.shape(img)[1]\n",
        "    im_width=tf.shape(img)[0]\n",
        "    \n",
        "    if n_hairs == 0:\n",
        "        return image\n",
        "\n",
        "    for _ in tf.range(n_hairs):\n",
        "\n",
        "        # Read a random hair image\n",
        "        i=tf.random.uniform(shape=[], maxval=tf.shape(hair_images_tf)[0], \n",
        "                            dtype=tf.int32)\n",
        "        fname=hair_images_tf[i]\n",
        "\n",
        "        bits = tf.io.read_file(fname)\n",
        "        hair = tf.image.decode_jpeg(bits)\n",
        "        \n",
        "        # Rescale the hair image to the right size (256 -- original size)\n",
        "        new_width=scale*tf.shape(hair)[1]\n",
        "        new_height=scale*tf.shape(hair)[0]\n",
        "        hair = tf.image.resize(hair, [new_height, new_width])\n",
        "\n",
        "        \n",
        "        # Random flips of the hair image\n",
        "        hair = tf.image.random_flip_left_right(hair)\n",
        "        hair = tf.image.random_flip_up_down(hair)\n",
        "        # Random number of 90 degree rotations\n",
        "        n_rot=tf.random.uniform(shape=[], maxval=4,\n",
        "                                dtype=tf.int32)\n",
        "        hair = tf.image.rot90(hair, k=n_rot)\n",
        "        \n",
        "        h_height=tf.shape(hair)[0]\n",
        "        h_width=tf.shape(hair)[1]\n",
        "        \n",
        "        roi_h0 = tf.random.uniform(shape=[], maxval=im_height - h_height + 1, \n",
        "                                    dtype=tf.int32)\n",
        "        roi_w0 = tf.random.uniform(shape=[], maxval=im_width - h_width + 1, \n",
        "                                    dtype=tf.int32)\n",
        "\n",
        "\n",
        "        roi = img[roi_h0:(roi_h0 + h_height), roi_w0:(roi_w0 + h_width)]  \n",
        "\n",
        "        # Convert the hair image to grayscale \n",
        "        # (slice to remove the trainsparency channel)\n",
        "        hair2gray = tf.image.rgb_to_grayscale(hair[:, :, :3])\n",
        "\n",
        "        mask=hair2gray>10\n",
        "\n",
        "        img_bg = tf.multiply(roi, tf.cast(tf.image.grayscale_to_rgb(~mask),\n",
        "                                          dtype=tf.float32))\n",
        "        hair_fg = tf.multiply(tf.cast(hair[:, :, :3], dtype=tf.int32),\n",
        "                              tf.cast(tf.image.grayscale_to_rgb(mask), \n",
        "                                      dtype=tf.int32\n",
        "                                      )\n",
        "                             )\n",
        "\n",
        "        dst = tf.add(img_bg, tf.cast(hair_fg, dtype=tf.float32)/255)\n",
        "\n",
        "        paddings = tf.stack([\n",
        "            [roi_h0, im_height-(roi_h0 + h_height)], \n",
        "            [roi_w0, im_width-(roi_w0 + h_width)],\n",
        "            [0, 0]\n",
        "        ])\n",
        "\n",
        "        # Pad dst with zeros to make it the same shape as image.\n",
        "        dst_padded=tf.pad(dst, paddings, \"CONSTANT\")\n",
        "        # Create a boolean mask with zeros at the pixels of\n",
        "        # the augmentation segment and ones everywhere else\n",
        "        mask_img=tf.pad(tf.ones_like(dst), paddings, \"CONSTANT\")\n",
        "        mask_img=~tf.cast(mask_img, dtype=tf.bool)\n",
        "        # Make a hole in the original image at the location\n",
        "        # of the augmentation segment\n",
        "        img_hole=tf.multiply(img, tf.cast(mask_img, dtype=tf.float32))\n",
        "        # Inserting the augmentation segment in place of the hole\n",
        "        img=tf.add(img_hole, dst_padded)\n",
        "        \n",
        "    return img"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBXawkxIxjY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def norm_image(data, label):\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "    data['inp1'] -= mean\n",
        "    data['inp1'] /= std\n",
        "    # mean_tensor = K.constant(-np.array(mean), tf.float32)\n",
        "    # std_tensor = K.constant(np.array(std), tf.float32)\n",
        "    # img = K.bias_add(img, mean_tensor, \"channels_last\")\n",
        "    # img /= std_tensor\n",
        "    return data, label"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca4n4VRcxpK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDCeVxZYvM4Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5409afb-33a3-4d24-8371-21337a00bfd4"
      },
      "source": [
        "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
        "    # returns 3x3 transformmatrix which transforms indicies\n",
        "        \n",
        "    # CONVERT DEGREES TO RADIANS\n",
        "    rotation = math.pi * rotation / 180.\n",
        "    shear = math.pi * shear / 180.\n",
        "    \n",
        "    # ROTATION MATRIX\n",
        "    c1 = tf.math.cos(rotation)\n",
        "    s1 = tf.math.sin(rotation)\n",
        "    one = tf.constant([1],dtype='float32')\n",
        "    zero = tf.constant([0],dtype='float32')\n",
        "    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n",
        "        \n",
        "    # SHEAR MATRIX\n",
        "    c2 = tf.math.cos(shear)\n",
        "    s2 = tf.math.sin(shear)\n",
        "    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n",
        "    \n",
        "    # ZOOM MATRIX\n",
        "    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n",
        "    \n",
        "    # SHIFT MATRIX\n",
        "    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n",
        "    \n",
        "    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n",
        "\n",
        "def transform(image, label):\n",
        "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
        "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
        "    DIM = img_size\n",
        "    XDIM = DIM%2 #fix for size 331\n",
        "    \n",
        "    if 0.5 > tf.random.uniform([1], minval = 0, maxval = 1):\n",
        "        rot = 15. * tf.random.normal([1],dtype='float32')\n",
        "    else:\n",
        "        rot = 180. * tf.random.normal([1],dtype='float32')\n",
        "    shr = 5. * tf.random.normal([1],dtype='float32') \n",
        "    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n",
        "    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n",
        "    h_shift = 16. * tf.random.normal([1],dtype='float32') \n",
        "    w_shift = 16. * tf.random.normal([1],dtype='float32') \n",
        "  \n",
        "    # GET TRANSFORMATION MATRIX\n",
        "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
        "\n",
        "    # LIST DESTINATION PIXEL INDICES\n",
        "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
        "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
        "    z = tf.ones([DIM*DIM],dtype='int32')\n",
        "    idx = tf.stack( [x,y,z] )\n",
        "    \n",
        "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
        "    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n",
        "    idx2 = K.cast(idx2,dtype='int32')\n",
        "    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
        "    \n",
        "    # FIND ORIGIN PIXEL VALUES           \n",
        "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
        "    d = tf.gather_nd(image['inp1'],tf.transpose(idx3))\n",
        "        \n",
        "    return {'inp1': tf.reshape(d,[DIM,DIM,3]), 'inp2': image['inp2']}, label\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "# function to decode our images (normalize and reshape)\n",
        "def decode_image(image_data):\n",
        "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "    # convert image to floats in [0, 1] range\n",
        "    image = tf.cast(image, tf.float32) / 255.0 \n",
        "    # explicit size needed for TPU\n",
        "    image = tf.reshape(image, [img_read_size, img_read_size, 3])\n",
        "\n",
        "    image = tf.image.resize(image, [img_size, img_size])\n",
        "    return image\n",
        "\n",
        "# this function parse our images and also get the target variable\n",
        "def read_labeled_tfrecord(example):\n",
        "    LABELED_TFREC_FORMAT = {\n",
        "        # tf.string means bytestring\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), \n",
        "        # shape [] means single element\n",
        "        \"target\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        # meta features\n",
        "        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"sex\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64)\n",
        "        \n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
        "    image = decode_image(example['image'])\n",
        "    label = tf.cast(example['target'], tf.int32)\n",
        "    # meta features\n",
        "    data = {}\n",
        "    data['age_approx'] = tf.cast(example['age_approx'], tf.int32)\n",
        "    data['sex'] = tf.cast(example['sex'], tf.int32)\n",
        "    data['anatom_site_general_challenge'] = tf.cast(tf.one_hot(example['anatom_site_general_challenge'], 7), tf.int32)\n",
        "    # returns a dataset of (image, label, data)\n",
        "    return image, label, data\n",
        "\n",
        "# this function parse our image and also get our image_name (id) to perform predictions\n",
        "def read_unlabeled_tfrecord(example):\n",
        "    UNLABELED_TFREC_FORMAT = {\n",
        "        # tf.string means bytestring\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), \n",
        "        # shape [] means single element\n",
        "        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
        "        # meta features\n",
        "        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"sex\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64)\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n",
        "    image = decode_image(example['image'])\n",
        "    image_name = example['image_name']\n",
        "    # meta features\n",
        "    data = {}\n",
        "    data['age_approx'] = tf.cast(example['age_approx'], tf.int32)\n",
        "    data['sex'] = tf.cast(example['sex'], tf.int32)\n",
        "    data['anatom_site_general_challenge'] = tf.cast(tf.one_hot(example['anatom_site_general_challenge'], 7), tf.int32)\n",
        "    # returns a dataset of (image, key, data)\n",
        "    return image, image_name, data\n",
        "    \n",
        "def load_dataset(filenames, labeled = True, ordered = False):\n",
        "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
        "    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n",
        "    \n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        # disable order, increase speed\n",
        "        ignore_order.experimental_deterministic = False \n",
        "        \n",
        "    # automatically interleaves reads from multiple files\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "    # use data as soon as it streams in, rather than in its original order\n",
        "    dataset = dataset.with_options(ignore_order)\n",
        "    # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n",
        "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO) \n",
        "    return dataset\n",
        "\n",
        "# function for training and validation dataset\n",
        "def setup_input1(image, label, data):\n",
        "    \n",
        "    # get anatom site general challenge vectors\n",
        "    anatom = [tf.cast(data['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\n",
        "    \n",
        "    tab_data = [tf.cast(data[tfeat], dtype = tf.float32) for tfeat in ['age_approx', 'sex']]\n",
        "    \n",
        "    tabular = tf.stack(tab_data + anatom)\n",
        "    \n",
        "    return {'inp1': image, 'inp2':  tabular}, label\n",
        "\n",
        "# function for the test set\n",
        "def setup_input2(image, image_name, data):\n",
        "    \n",
        "    # get anatom site general challenge vectors\n",
        "    anatom = [tf.cast(data['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\n",
        "    \n",
        "    tab_data = [tf.cast(data[tfeat], dtype = tf.float32) for tfeat in ['age_approx', 'sex']]\n",
        "    \n",
        "    tabular = tf.stack(tab_data + anatom)\n",
        "    \n",
        "    return {'inp1': image, 'inp2':  tabular}, image_name\n",
        "\n",
        "# function for the validation (image name)\n",
        "def setup_input3(image, image_name, target, data):\n",
        "    \n",
        "    # get anatom site general challenge vectors\n",
        "    anatom = [tf.cast(data['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\n",
        "    \n",
        "    tab_data = [tf.cast(data[tfeat], dtype = tf.float32) for tfeat in ['age_approx', 'sex']]\n",
        "    \n",
        "    tabular = tf.stack(tab_data + anatom)\n",
        "    \n",
        "    return {'inp1': image, 'inp2':  tabular}, image_name, target\n",
        "\n",
        "def data_augment(data, label):\n",
        "    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement \n",
        "    # in the next function (below), this happens essentially for free on TPU. \n",
        "    # Data pipeline code is executed on the \"CPU\" part\n",
        "    # of the TPU while the TPU itself is computing gradients.\n",
        "    data['inp1'] = hair_aug(data['inp1'])\n",
        "    data['inp1'] = tf.image.random_flip_left_right(data['inp1'])\n",
        "    data['inp1'] = tf.image.random_flip_up_down(data['inp1'])\n",
        "    #data['inp1'] = tf.image.rot90(data['inp1'],90)\n",
        "    #data['inp1'] = tf.image.transpose(data['inp1'])\n",
        "    \n",
        "    #data['inp1'] = random_cuout_aug(data['inp1'])\n",
        "    #data['inp1'] = tf.keras.preprocessing.image.random_shear(data['inp1'],20)\n",
        "    #data['inp1'] = tf.keras.preprocessing.image.random_shift(data['inp1'],0.1,0.1)\n",
        "    #data['inp1'] = cutout_aug(data['inp1'])\n",
        "    \n",
        "    data['inp1'] = tf.image.random_hue(data['inp1'], 0.01)\n",
        "    data['inp1'] = tf.image.random_saturation(data['inp1'], 0.7, 1.3)\n",
        "    data['inp1'] = tf.image.random_contrast(data['inp1'], 0.8, 1.2)\n",
        "    data['inp1'] = tf.image.random_brightness(data['inp1'], 0.1)\n",
        "    \n",
        "    \n",
        "    return data, label\n",
        "\n",
        "def get_training_dataset(filenames, labeled = True, ordered = False):\n",
        "    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n",
        "    dataset = dataset.map(setup_input1, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(transform, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(norm_image, num_parallel_calls = AUTO)\n",
        "    # the training dataset must repeat for several epochs\n",
        "    dataset = dataset.repeat() \n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "def get_validation_dataset(filenames, labeled = True, ordered = True):\n",
        "    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n",
        "    dataset = dataset.map(setup_input1, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(norm_image, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    # using gpu, not enought memory to use cache\n",
        "    # dataset = dataset.cache()\n",
        "    # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    dataset = dataset.prefetch(AUTO) \n",
        "    return dataset\n",
        "\n",
        "def get_test_dataset(filenames, labeled = False, ordered = True):\n",
        "    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n",
        "    dataset = dataset.map(setup_input2, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(norm_image, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    dataset = dataset.prefetch(AUTO) \n",
        "    return dataset\n",
        "\n",
        "# function to count how many photos we have in\n",
        "def count_data_items(filenames):\n",
        "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    return np.sum(n)\n",
        "\n",
        "# this function parse our images and also get the target variable\n",
        "def read_tfrecord_full(example):\n",
        "    LABELED_TFREC_FORMAT = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), \n",
        "        \"image_name\": tf.io.FixedLenFeature([], tf.string), \n",
        "        \"target\": tf.io.FixedLenFeature([], tf.int64), \n",
        "        # meta features\n",
        "        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"sex\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64)\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
        "    image = decode_image(example['image'])\n",
        "    image_name = example['image_name']\n",
        "    target = tf.cast(example['target'], tf.int32)\n",
        "    # meta features\n",
        "    data = {}\n",
        "    data['age_approx'] = tf.cast(example['age_approx'], tf.int32)\n",
        "    data['sex'] = tf.cast(example['sex'], tf.int32)\n",
        "    data['anatom_site_general_challenge'] = tf.cast(tf.one_hot(example['anatom_site_general_challenge'], 7), tf.int32)\n",
        "    return image, image_name, target, data\n",
        "\n",
        "def load_dataset_full(filenames):        \n",
        "    # automatically interleaves reads from multiple files\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "    # returns a dataset of (image_name, target)\n",
        "    dataset = dataset.map(read_tfrecord_full, num_parallel_calls = AUTO) \n",
        "    return dataset\n",
        "\n",
        "def get_data_full(filenames):\n",
        "    dataset = load_dataset_full(filenames)\n",
        "    dataset = dataset.map(setup_input3, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "\n",
        "NUM_TRAINING_IMAGES = int(count_data_items(TRAINING_FILENAMES) * 0.8)\n",
        "# use validation data for training\n",
        "NUM_VALIDATION_IMAGES = int(count_data_items(TRAINING_FILENAMES) * 0.2)\n",
        "NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n",
        "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
        "\n",
        "print('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 36440 training images, 9110 validation images, 10982 unlabeled test images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBUUjJ9t0VII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_DECAY = 0.0001\n",
        "\n",
        "\n",
        "class AttentionModel(tf.keras.Model):\n",
        "  \"\"\"Instantiates attention model.\n",
        "  Uses two [kernel_size x kernel_size] convolutions and softplus as activation\n",
        "  to compute an attention map with the same resolution as the featuremap.\n",
        "  Features l2-normalized and aggregated using attention probabilites as weights.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, kernel_size=1, decay=_DECAY, name='attention'):\n",
        "    \"\"\"Initialization of attention model.\n",
        "    Args:\n",
        "      kernel_size: int, kernel size of convolutions.\n",
        "      decay: float, decay for l2 regularization of kernel weights.\n",
        "      name: str, name to identify model.\n",
        "    \"\"\"\n",
        "    super(AttentionModel, self).__init__(name=name)\n",
        "\n",
        "    # First convolutional layer (called with relu activation).\n",
        "    self.conv1 = tf.keras.layers.Conv2D(\n",
        "        512,\n",
        "        kernel_size,\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(decay),\n",
        "        padding='same',\n",
        "        name='attn_conv1')\n",
        "    self.bn_conv1 = tf.keras.layers.BatchNormalization(axis=3, name='bn_conv1')\n",
        "\n",
        "    # Second convolutional layer, with softplus activation.\n",
        "    self.conv2 = tf.keras.layers.Conv2D(\n",
        "        1,\n",
        "        kernel_size,\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(decay),\n",
        "        padding='same',\n",
        "        name='attn_conv2')\n",
        "    self.activation_layer = tf.keras.layers.Activation('softplus')\n",
        "\n",
        "  def call(self, inputs, training=True):\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.bn_conv1(x, training=training)\n",
        "    x = tf.nn.relu(x)\n",
        "\n",
        "    score = self.conv2(x)\n",
        "    prob = self.activation_layer(score)\n",
        "\n",
        "    # L2-normalize the featuremap before pooling.\n",
        "    inputs = tf.nn.l2_normalize(inputs, axis=-1)\n",
        "    feat = tf.multiply(inputs, prob)\n",
        "    # feat = tf.reduce_mean(tf.multiply(inputs, prob), [1, 2], keepdims=False)\n",
        "\n",
        "    return feat, prob, score"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7QP9w97vM4a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f06a787e-33a1-4618-d662-6fdcdaf88435"
      },
      "source": [
        "def binary_focal_loss(gamma=2., alpha=.25):\n",
        "    \"\"\"\n",
        "    Binary form of focal loss.\n",
        "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
        "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
        "    References:\n",
        "        https://arxiv.org/pdf/1708.02002.pdf\n",
        "    Usage:\n",
        "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
        "    \"\"\"\n",
        "    def binary_focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        :param y_true: A tensor of the same shape as `y_pred`\n",
        "        :param y_pred:  A tensor resulting from a sigmoid\n",
        "        :return: Output tensor.\n",
        "        \"\"\"\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        epsilon = K.epsilon()\n",
        "        # clip to prevent NaN's and Inf's\n",
        "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
        "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
        "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
        "\n",
        "    return binary_focal_loss_fixed\n",
        "\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    with strategy.scope():\n",
        "        inp1 = tf.keras.layers.Input(shape = (img_size, img_size, 3), name = 'inp1')\n",
        "        inp2 = tf.keras.layers.Input(shape = (9), name = 'inp2')\n",
        "        efnetb5 = efn.EfficientNetB5(weights = 'imagenet', include_top = False)\n",
        "        x = efnetb5(inp1)\n",
        "        attention_model = AttentionModel(name='attention')\n",
        "        x, _, _ = attention_model(x)\n",
        "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "        x1 = tf.keras.layers.Dense(256)(inp2)\n",
        "        x1 = tf.keras.layers.BatchNormalization()(x1)\n",
        "        x1 = tf.keras.layers.Dropout(0.2)(x1)\n",
        "        x1 = tf.keras.layers.Dense(256)(inp2)\n",
        "        x1 = tf.keras.layers.Dropout(0.2)(x1)\n",
        "        x1 = tf.keras.layers.Activation('relu')(x1)\n",
        "        x1 = tf.keras.layers.BatchNormalization()(x1)\n",
        "        concat = tf.keras.layers.concatenate([x, x1])\n",
        "        #concat = tf.keras.layers.Dense(512, activation = 'relu')(concat)\n",
        "        #concat = tf.keras.layers.BatchNormalization()(concat)\n",
        "        #concat = tf.keras.layers.Dropout(0.2)(concat)\n",
        "        #concat = tf.keras.layers.Dense(182, activation = 'relu')(concat)\n",
        "        #concat = tf.keras.layers.BatchNormalization()(concat)\n",
        "        #concat = tf.keras.layers.Dropout(0.2)(concat)\n",
        "        output = tf.keras.layers.Dense(1, activation = 'sigmoid')(concat)\n",
        "\n",
        "        model = tf.keras.models.Model(inputs = [inp1, inp2], outputs = [output])\n",
        "\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate = LR)\n",
        "        opt = tfa.optimizers.RectifiedAdam(lr=LR)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer = opt,\n",
        "            # loss = [tfa.losses.SigmoidFocalCrossEntropy(gamma = 2.0, alpha = 0.80)],\n",
        "            loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05),\n",
        "            metrics = [tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()]\n",
        "        )\n",
        "\n",
        "        return model\n",
        "    \n",
        "def train_and_predict(SUB, folds = 3):\n",
        "    \n",
        "    # models = []\n",
        "    oof_image_name = []\n",
        "    oof_target = []\n",
        "    oof_prediction = []\n",
        "    \n",
        "    # seed everything\n",
        "    seed_everything(SEED)\n",
        "\n",
        "    kfold = KFold(folds, shuffle = True, random_state = SEED)\n",
        "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(TRAINING_FILENAMES)):\n",
        "        print('\\n')\n",
        "        print('-'*50)\n",
        "        print(f'Training fold {fold + 1}')\n",
        "        train_dataset = get_training_dataset([TRAINING_FILENAMES[x] for x in trn_ind], labeled = True, ordered = False)\n",
        "        val_dataset = get_validation_dataset([TRAINING_FILENAMES[x] for x in val_ind], labeled = True, ordered = True)\n",
        "        K.clear_session()\n",
        "        model = get_model()\n",
        "        # using early stopping using val loss\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_auc', mode = 'max', patience = 5, \n",
        "                                                      verbose = 1, min_delta = 0.0001, restore_best_weights = True)\n",
        "        # lr scheduler\n",
        "        cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_auc', factor = 0.4, patience = 2, verbose = 1, min_delta = 0.0001, mode = 'max')\n",
        "        sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "            os.path.join(outdir,'fold-%i.h5'%fold), monitor='val_loss', verbose=0, save_best_only=True,\n",
        "            save_weights_only=True, mode='min', save_freq='epoch')\n",
        "    \n",
        "        svauc = tf.keras.callbacks.ModelCheckpoint(\n",
        "            os.path.join(outdir,'fold-%i-bestvalauc.h5'%fold), monitor='val_auc', verbose=0, save_best_only=True,\n",
        "            save_weights_only=True, mode='max', save_freq='epoch')\n",
        "        if fold >=3:\n",
        "            history = model.fit(train_dataset, \n",
        "                                steps_per_epoch = STEPS_PER_EPOCH,\n",
        "                                epochs = EPOCHS,\n",
        "                                callbacks = [sv, svauc, early_stopping, cb_lr_schedule],\n",
        "                                validation_data = val_dataset,\n",
        "                                verbose = 2)\n",
        "        model.load_weights(os.path.join(outdir,'fold-%i.h5'%fold))\n",
        "        # models.append(model)\n",
        "        \n",
        "        # want to predict the validation set and save them for stacking\n",
        "        number_of_files = count_data_items([TRAINING_FILENAMES[x] for x in val_ind])\n",
        "        dataset = get_data_full([TRAINING_FILENAMES[x] for x in val_ind])\n",
        "        # get the image name\n",
        "        image_name = dataset.map(lambda image, image_name, target: image_name).unbatch()\n",
        "        image_name = next(iter(image_name.batch(number_of_files))).numpy().astype('U')\n",
        "        # get the real target\n",
        "        target = dataset.map(lambda image, image_name, target: target).unbatch()\n",
        "        target = next(iter(target.batch(number_of_files))).numpy()\n",
        "        # predict the validation set\n",
        "        image = dataset.map(lambda image, image_name, target: image)\n",
        "        probabilities = model.predict(image)\n",
        "        oof_image_name.extend(list(image_name))\n",
        "        oof_target.extend(list(target))\n",
        "        oof_prediction.extend(list(np.concatenate(probabilities)))\n",
        "    \n",
        "    print('\\n')\n",
        "    print('-'*50)\n",
        "    # save oof predictions\n",
        "    oof_df = pd.DataFrame({'image_name': oof_image_name, 'target': oof_target, 'predictions': oof_prediction})\n",
        "    oof_df.to_csv(os.path.join(outdir,'OOF_EfficientNetB5_{}.csv'.format(img_size)), index = False)\n",
        "        \n",
        "    # since we are splitting the dataset and iterating separately on images and ids, order matters.\n",
        "    test_ds = get_test_dataset(TEST_FILENAMES, labeled = False, ordered = True)\n",
        "    test_images_ds = test_ds.map(lambda image, image_name: image)\n",
        "    \n",
        "    print('Computing predictions...')\n",
        "    preds = []\n",
        "    for i in range(folds):\n",
        "        print('Loading best model...')\n",
        "        model.load_weights(os.path.join(outdir,'fold-%i.h5'%i))\n",
        "        preds.append([model.predict(test_images_ds)])\n",
        "    probabilities = np.average(np.squeeze(np.concatenate(preds)), axis = 0)\n",
        "    print('Generating submission.csv file...')\n",
        "    test_ids_ds = test_ds.map(lambda image, image_name: image_name).unbatch()\n",
        "    # all in one batch\n",
        "    test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n",
        "    pred_df = pd.DataFrame({'image_name': test_ids, 'target': probabilities})\n",
        "    SUB.drop('target', inplace = True, axis = 1)\n",
        "    SUB = SUB.merge(pred_df, on = 'image_name')\n",
        "    SUB.to_csv(os.path.join(outdir,'sub_EfficientNetB5_{}.csv'.format(img_size)), index = False)\n",
        "    \n",
        "    return oof_target, oof_prediction\n",
        "    \n",
        "oof_target, oof_prediction = train_and_predict(SUB, folds=5)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Training fold 1\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Training fold 2\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Training fold 3\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Training fold 4\n",
            "Epoch 1/40\n",
            "569/569 - 317s - auc: 0.7855 - loss: 0.3129 - binary_accuracy: 0.9309 - val_auc: 0.8676 - val_loss: 0.3361 - val_binary_accuracy: 0.9091 - lr: 3.0000e-04\n",
            "Epoch 2/40\n",
            "569/569 - 289s - auc: 0.8632 - loss: 0.2263 - binary_accuracy: 0.9570 - val_auc: 0.8917 - val_loss: 0.2821 - val_binary_accuracy: 0.9121 - lr: 3.0000e-04\n",
            "Epoch 3/40\n",
            "569/569 - 271s - auc: 0.8774 - loss: 0.2140 - binary_accuracy: 0.9576 - val_auc: 0.8906 - val_loss: 0.2831 - val_binary_accuracy: 0.9239 - lr: 3.0000e-04\n",
            "Epoch 4/40\n",
            "569/569 - 291s - auc: 0.8880 - loss: 0.2079 - binary_accuracy: 0.9598 - val_auc: 0.9028 - val_loss: 0.2682 - val_binary_accuracy: 0.9239 - lr: 3.0000e-04\n",
            "Epoch 5/40\n",
            "569/569 - 288s - auc: 0.8969 - loss: 0.2055 - binary_accuracy: 0.9606 - val_auc: 0.9160 - val_loss: 0.2650 - val_binary_accuracy: 0.9251 - lr: 3.0000e-04\n",
            "Epoch 6/40\n",
            "569/569 - 271s - auc: 0.9028 - loss: 0.2019 - binary_accuracy: 0.9621 - val_auc: 0.8980 - val_loss: 0.2700 - val_binary_accuracy: 0.9248 - lr: 3.0000e-04\n",
            "Epoch 7/40\n",
            "569/569 - 291s - auc: 0.9046 - loss: 0.1998 - binary_accuracy: 0.9629 - val_auc: 0.9256 - val_loss: 0.2600 - val_binary_accuracy: 0.9240 - lr: 3.0000e-04\n",
            "Epoch 8/40\n",
            "569/569 - 276s - auc: 0.9111 - loss: 0.1931 - binary_accuracy: 0.9661 - val_auc: 0.9243 - val_loss: 0.2628 - val_binary_accuracy: 0.9246 - lr: 3.0000e-04\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00012000000569969416.\n",
            "569/569 - 282s - auc: 0.9123 - loss: 0.1936 - binary_accuracy: 0.9651 - val_auc: 0.9109 - val_loss: 0.2572 - val_binary_accuracy: 0.9300 - lr: 3.0000e-04\n",
            "Epoch 10/40\n",
            "569/569 - 295s - auc: 0.9350 - loss: 0.1874 - binary_accuracy: 0.9673 - val_auc: 0.9320 - val_loss: 0.2446 - val_binary_accuracy: 0.9359 - lr: 1.2000e-04\n",
            "Epoch 11/40\n",
            "569/569 - 294s - auc: 0.9427 - loss: 0.1816 - binary_accuracy: 0.9701 - val_auc: 0.9357 - val_loss: 0.2445 - val_binary_accuracy: 0.9358 - lr: 1.2000e-04\n",
            "Epoch 12/40\n",
            "569/569 - 288s - auc: 0.9432 - loss: 0.1816 - binary_accuracy: 0.9704 - val_auc: 0.9399 - val_loss: 0.2384 - val_binary_accuracy: 0.9395 - lr: 1.2000e-04\n",
            "Epoch 13/40\n",
            "569/569 - 266s - auc: 0.9519 - loss: 0.1764 - binary_accuracy: 0.9724 - val_auc: 0.9357 - val_loss: 0.2438 - val_binary_accuracy: 0.9350 - lr: 1.2000e-04\n",
            "Epoch 14/40\n",
            "569/569 - 294s - auc: 0.9517 - loss: 0.1765 - binary_accuracy: 0.9727 - val_auc: 0.9427 - val_loss: 0.2410 - val_binary_accuracy: 0.9376 - lr: 1.2000e-04\n",
            "Epoch 15/40\n",
            "569/569 - 265s - auc: 0.9558 - loss: 0.1727 - binary_accuracy: 0.9746 - val_auc: 0.9280 - val_loss: 0.2524 - val_binary_accuracy: 0.9351 - lr: 1.2000e-04\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 4.8000001697801054e-05.\n",
            "569/569 - 279s - auc: 0.9574 - loss: 0.1760 - binary_accuracy: 0.9725 - val_auc: 0.9296 - val_loss: 0.2420 - val_binary_accuracy: 0.9378 - lr: 1.2000e-04\n",
            "Epoch 17/40\n",
            "569/569 - 264s - auc: 0.9632 - loss: 0.1711 - binary_accuracy: 0.9748 - val_auc: 0.9328 - val_loss: 0.2431 - val_binary_accuracy: 0.9412 - lr: 4.8000e-05\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.920000067912042e-05.\n",
            "569/569 - 271s - auc: 0.9665 - loss: 0.1682 - binary_accuracy: 0.9765 - val_auc: 0.9374 - val_loss: 0.2396 - val_binary_accuracy: 0.9395 - lr: 4.8000e-05\n",
            "Epoch 19/40\n",
            "Restoring model weights from the end of the best epoch.\n",
            "569/569 - 293s - auc: 0.9709 - loss: 0.1645 - binary_accuracy: 0.9776 - val_auc: 0.9368 - val_loss: 0.2320 - val_binary_accuracy: 0.9432 - lr: 1.9200e-05\n",
            "Epoch 00019: early stopping\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Training fold 5\n",
            "Epoch 1/40\n",
            "569/569 - 335s - auc: 0.8003 - loss: 0.3299 - binary_accuracy: 0.9244 - val_auc: 0.9042 - val_loss: 0.2224 - val_binary_accuracy: 0.9646 - lr: 3.0000e-04\n",
            "Epoch 2/40\n",
            "569/569 - 307s - auc: 0.8729 - loss: 0.2457 - binary_accuracy: 0.9471 - val_auc: 0.8972 - val_loss: 0.1981 - val_binary_accuracy: 0.9653 - lr: 3.0000e-04\n",
            "Epoch 3/40\n",
            "569/569 - 300s - auc: 0.8899 - loss: 0.2281 - binary_accuracy: 0.9493 - val_auc: 0.9147 - val_loss: 0.2180 - val_binary_accuracy: 0.9680 - lr: 3.0000e-04\n",
            "Epoch 4/40\n",
            "569/569 - 291s - auc: 0.8949 - loss: 0.2217 - binary_accuracy: 0.9516 - val_auc: 0.9116 - val_loss: 0.1929 - val_binary_accuracy: 0.9689 - lr: 3.0000e-04\n",
            "Epoch 5/40\n",
            "569/569 - 310s - auc: 0.9041 - loss: 0.2157 - binary_accuracy: 0.9543 - val_auc: 0.9385 - val_loss: 0.1826 - val_binary_accuracy: 0.9708 - lr: 3.0000e-04\n",
            "Epoch 6/40\n",
            "569/569 - 291s - auc: 0.9101 - loss: 0.2134 - binary_accuracy: 0.9555 - val_auc: 0.9187 - val_loss: 0.1927 - val_binary_accuracy: 0.9673 - lr: 3.0000e-04\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00012000000569969416.\n",
            "569/569 - 293s - auc: 0.9165 - loss: 0.2098 - binary_accuracy: 0.9564 - val_auc: 0.9353 - val_loss: 0.1970 - val_binary_accuracy: 0.9619 - lr: 3.0000e-04\n",
            "Epoch 8/40\n",
            "569/569 - 299s - auc: 0.9349 - loss: 0.1999 - binary_accuracy: 0.9607 - val_auc: 0.9451 - val_loss: 0.1817 - val_binary_accuracy: 0.9695 - lr: 1.2000e-04\n",
            "Epoch 9/40\n",
            "569/569 - 297s - auc: 0.9417 - loss: 0.1941 - binary_accuracy: 0.9642 - val_auc: 0.9505 - val_loss: 0.1832 - val_binary_accuracy: 0.9689 - lr: 1.2000e-04\n",
            "Epoch 10/40\n",
            "569/569 - 294s - auc: 0.9455 - loss: 0.1928 - binary_accuracy: 0.9631 - val_auc: 0.9475 - val_loss: 0.1771 - val_binary_accuracy: 0.9724 - lr: 1.2000e-04\n",
            "Epoch 11/40\n",
            "569/569 - 297s - auc: 0.9489 - loss: 0.1894 - binary_accuracy: 0.9662 - val_auc: 0.9535 - val_loss: 0.1819 - val_binary_accuracy: 0.9700 - lr: 1.2000e-04\n",
            "Epoch 12/40\n",
            "569/569 - 301s - auc: 0.9571 - loss: 0.1853 - binary_accuracy: 0.9684 - val_auc: 0.9500 - val_loss: 0.1728 - val_binary_accuracy: 0.9746 - lr: 1.2000e-04\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.8000001697801054e-05.\n",
            "569/569 - 280s - auc: 0.9594 - loss: 0.1826 - binary_accuracy: 0.9690 - val_auc: 0.9493 - val_loss: 0.1754 - val_binary_accuracy: 0.9728 - lr: 1.2000e-04\n",
            "Epoch 14/40\n",
            "569/569 - 297s - auc: 0.9638 - loss: 0.1798 - binary_accuracy: 0.9704 - val_auc: 0.9447 - val_loss: 0.1746 - val_binary_accuracy: 0.9743 - lr: 4.8000e-05\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.920000067912042e-05.\n",
            "569/569 - 301s - auc: 0.9689 - loss: 0.1747 - binary_accuracy: 0.9728 - val_auc: 0.9423 - val_loss: 0.1759 - val_binary_accuracy: 0.9726 - lr: 4.8000e-05\n",
            "Epoch 16/40\n",
            "569/569 - 299s - auc: 0.9706 - loss: 0.1729 - binary_accuracy: 0.9740 - val_auc: 0.9547 - val_loss: 0.1735 - val_binary_accuracy: 0.9739 - lr: 1.9200e-05\n",
            "Epoch 17/40\n",
            "569/569 - 286s - auc: 0.9727 - loss: 0.1699 - binary_accuracy: 0.9750 - val_auc: 0.9486 - val_loss: 0.1737 - val_binary_accuracy: 0.9739 - lr: 1.9200e-05\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.680000271648168e-06.\n",
            "569/569 - 295s - auc: 0.9722 - loss: 0.1700 - binary_accuracy: 0.9759 - val_auc: 0.9527 - val_loss: 0.1723 - val_binary_accuracy: 0.9750 - lr: 1.9200e-05\n",
            "Epoch 19/40\n",
            "569/569 - 294s - auc: 0.9725 - loss: 0.1680 - binary_accuracy: 0.9764 - val_auc: 0.9498 - val_loss: 0.1733 - val_binary_accuracy: 0.9748 - lr: 7.6800e-06\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.0720002541784197e-06.\n",
            "569/569 - 293s - auc: 0.9739 - loss: 0.1667 - binary_accuracy: 0.9765 - val_auc: 0.9528 - val_loss: 0.1735 - val_binary_accuracy: 0.9741 - lr: 7.6800e-06\n",
            "Epoch 21/40\n",
            "Restoring model weights from the end of the best epoch.\n",
            "569/569 - 299s - auc: 0.9728 - loss: 0.1685 - binary_accuracy: 0.9760 - val_auc: 0.9532 - val_loss: 0.1738 - val_binary_accuracy: 0.9743 - lr: 3.0720e-06\n",
            "Epoch 00021: early stopping\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Computing predictions...\n",
            "Loading best model...\n",
            "Loading best model...\n",
            "Loading best model...\n",
            "Loading best model...\n",
            "Loading best model...\n",
            "Generating submission.csv file...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iR9KPUUxDNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyZpLdjBvM4c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "beb3c034-17c1-4010-d6bf-5464610bc69d"
      },
      "source": [
        "# calculate our out of folds roc auc score\n",
        "roc_auc = metrics.roc_auc_score(oof_target, oof_prediction)\n",
        "print('Our out of folds roc auc score is: ', roc_auc)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our out of folds roc auc score is:  0.815879943456897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG8U-HM0vM4e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83705abd-186a-4f3d-85b8-8d99bcdf7a78"
      },
      "source": [
        "print(outdir)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "siim/res/efficientnet-res/EF5-512/all-kfold-hairaug-456-norm-meta-dense\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nnCl06xvM4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}